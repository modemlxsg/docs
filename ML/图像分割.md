# 图像分割

## 概述

图像分割是预测图像中每一个像素所属的类别或者物体。基于深度学习的图像分割算法主要分为两类：

**1、语义分割 semantic segmentation**

为图像中的每个像素分配一个类别，如把画面中的所有物体都指出它们各自的类别。

**2、实例分割 instance segmentation**

与语义分割不同，实例分割只对特定物体进行类别分配，这一点与目标检测有点相似，但目标检测输出的是边界框和类别，而实例分割输出的是掩膜（mask）和类别。

**3、全景分割 panoptic segmentation**

同时实现实例分割与不可数类别的语义分割

![image-20200111060522655](images\图像分割.assets\image-20200111060522655.png)



**4、图像识别、目标检测、语义分割、实例分割**

![image-20200110031703649](images\图像分割.assets\image-20200110031703649.png)

**5、难度**

**图像识别 < 语义分割 < 目标检测 < 实例分割**



## 上采样

**UpPooling**

![image-20200111062500842](images\图像分割.assets\image-20200111062500842.png)



**UpSampling**

![image-20200111062513444](images\图像分割.assets\image-20200111062513444.png)



**Deconvolution**

![image-20200111062543284](images\图像分割.assets\image-20200111062543284.png)



## 语义分割

***

语义图像分割的任务是从一组预定义的类中对图像中的每个像素进行分类。在下面的示例中，对不同的实体进行分类。

![image-20200110030300693](images\图像分割.assets\image-20200110030300693.png)

在上面的例子中，属于bed的像素被划分为“bed”类，与wall对应的像素被标记为“wall”等。

特别地，我们的目标是取一个大小为**WxHx3**的图像，并生成一个包含与所有像素对应的预测类ID的**WxH**矩阵。

![image-20200110030437474](images\图像分割.assets\image-20200110030437474.png)

通常，在一个有各种实体的图像中，我们想知道哪个像素属于哪个实体，例如在一个户外图像中，我们可以分割天空，地面，树木，人等等。

语义分割不同于对象检测，因为它不预测对象周围的任何边界框。我们**不区分同一对象的不同实例**。例如，场景中可能有多辆车，它们都有相同的标签。



**模型选择：**

对于**室内和室外场景**的图像，**PSPNet**是首选的，因为对象通常以不同的大小出现。这里的模型输入大小应该相当大，大约是500x500。

对于**医学领域**的图像，**UNet**是最受欢迎的选择。由于跳过连接，UNet不会错过这些微小的细节。UNet也可用于小型物体的室内/室外场景。

对于**简单的数据集**，具有较大的大小和少量的对象，UNet和PSPNet可能是一个过猛的选择。在这里，像**FCN或Segnet**这样的简单模型就足够了。



### FCN

***

FCN是最早提出的端到端语义分割模型之一。在这里，通过对FC层进行1x1卷积，将VGG和AlexNet等标准图像分类模型转换为全卷积。在FCN中，反卷积用于上采样，不像其他使用数学插值的方法。三种改型是FCN8、FCN16和FCN32。在FCN8和FCN16中，使用跳过连接。

FCN对图像进行像素级的分类，从而解决了语义级别的图像分割问题。与经典的CNN在卷积层使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用**反卷积层**对最后一个卷基层的特征图（feature map）进行**上采样**，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图进行像素的分类。

![image-20200110034647911](images\图像分割.assets\image-20200110034647911.png)

![image-20200111032654911](images\图像分割.assets\image-20200111032654911.png)

1. image经过多个conv和+max pooling变为pool1_feature，宽高变为1/2
2. pool1_feature再经过多个conv+max pooling变为pool2_feature，宽高变为1/4
3. pool2 feature再经过多个conv+一个max pooling变为pool3 feature，宽高变为1/8
4. ......
5. 直到pool5_feature，宽高变为1/32。

那么：

1. 对于**FCN-32s**，直接对pool5_feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即分割图）。
2. 对于**FCN-16s**，首先对pool5_feature进行2倍上采样获得2x upsampled feature，再把pool4_feature和2x upsampled feature**逐点相加**，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。
3. 对于**FCN-8s**，首先进行pool4+2x upsampled feature**逐点相加**，然后又进行pool3+2x upsampled**逐点相加**，即进行更多次特征融合。具体过程与16s类似，不再赘述。
4. FCN-32s < FCN-16s < FCN-8s，即**使用多层feature融合有利于提高分割准确性**。

![image-20200111033111818](images\图像分割.assets\image-20200111033111818.png)



### SegNet

***

SegNet架构采用编解码器框架。编码器和解码器层彼此对称。**解码器层的上采样操作使用相应编码器层的最大池索引**。SegNet没有任何跳过连接。与FCN不同，向上采样不使用可学习参数。

**uppooling**

![image-20200110034733206](images\图像分割.assets\image-20200110034733206.png)

![image-20200111051814403](images\图像分割.assets\image-20200111051814403.png)

```python
img_input = Input(shape=(224,224,3))

# Block 1
x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last')(img_input)
x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format='channels_last')(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format='channels_last' )(x)
f1 = x

# Block 2
x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format='channels_last' )(x)
x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format='channels_last' )(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last' )(x)
f2 = x

# Block 3
x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format='channels_last' )(x)
x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format='channels_last' )(x)
x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format='channels_last' )(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format='channels_last' )(x)
f3 = x

# Block 4
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format='channels_last' )(x)
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format='channels_last' )(x)
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format='channels_last' )(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format='channels_last' )(x)
f4 = x

# Block 5
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format='channels_last' )(x)
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format='channels_last' )(x)
x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format='channels_last' )(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format='channels_last' )(x)
f5 = x

x = Flatten(name='flatten')(x)
x = Dense(4096, activation='relu', name='fc1')(x)
x = Dense(4096, activation='relu', name='fc2')(x)
x = Dense( 1000 , activation='softmax', name='predictions')(x)

levels = [f1 , f2 , f3 , f4 , f5 ]

o = levels[ 3 ] # (None,14,14,512)

o = ZeroPadding2D( (1,1) , data_format='channels_last' )(o) # (None,16,16,512)
o = Conv2D(512, (3, 3), padding='valid', data_format='channels_last')(o)
o = BatchNormalization()(o) # (None,14,14,512)

o = UpSampling2D( (2,2), data_format='channels_last')(o) # (None, 28, 28, 512)
o = ZeroPadding2D( (1,1), data_format='channels_last')(o)
o = Conv2D( 512, (3, 3), padding='valid', data_format='channels_last')(o)
o = BatchNormalization()(o) #(None, 28, 28, 512)

o = UpSampling2D( (2,2), data_format='channels_last')(o) # (None, 56, 56, 512)
o = ZeroPadding2D( (1,1), data_format='channels_last')(o)
o = Conv2D( 256, (3, 3), padding='valid', data_format='channels_last')(o)
o = BatchNormalization()(o) # (None, 56, 56, 256)

o = UpSampling2D((2,2)  , data_format='channels_last' )(o) #(None, 112, 112, 256)
o = ZeroPadding2D((1,1) , data_format='channels_last' )(o)
o = Conv2D( 128 , (3, 3), padding='valid' , data_format='channels_last')(o)
o = BatchNormalization()(o) #(None, 112, 112, 128)

o = UpSampling2D((2,2)  , data_format='channels_last' )(o) #(None, 224, 224, 128)
o = ZeroPadding2D((1,1)  , data_format='channels_last' )(o)
o = Conv2D( 64 , (3, 3), padding='valid'  , data_format='channels_last')(o)
o = BatchNormalization()(o) # (None, 224, 224, 64)

o = Conv2D(21, (3, 3) , padding='same', data_format='channels_last')( o ) # (None, 224, 224, 21)
o = Activation('softmax')(o)

model = keras.models.Model(inputs=img_input, outputs=o)
model.summary()
```









### UNet

***

U-Net作者提出能够适应很小的训练集（大约30张图）。U-Net与FCN都是很小的分割网络，既没有使用**空洞卷积**，也没有后接**CRF**，结构简单

UNet体系结构采用带跳过连接的编解码器框架。像SegNet一样，编码器和解码器层是对称的。

![image-20200111043448431](images\图像分割.assets\image-20200111043448431.png)

```python
n_filters = 16
inputs = Input(shape=(256, 256, 3))

conv1 = Conv2D(n_filters * 1, (3, 3), activation='relu', padding = 'same')(inputs)
conv1 = BatchNormalization()(conv1)
conv1 = Conv2D(n_filters * 1, (3, 3), activation='relu', padding = 'same')(conv1)
conv1 = BatchNormalization()(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2), data_format='channels_last')(conv1)

conv2 = Conv2D(n_filters * 2, (3, 3), activation='relu', padding = 'same')(pool1)
conv2 = BatchNormalization()(conv2)
conv2 = Conv2D(n_filters * 2, (3, 3), activation='relu', padding = 'same')(conv2)
conv2 = BatchNormalization()(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2), data_format='channels_last')(conv2)

conv3 = Conv2D(n_filters * 4, (3, 3), activation='relu', padding = 'same')(pool2)
conv3 = BatchNormalization()(conv3)
conv3 = Conv2D(n_filters * 4, (3, 3), activation='relu', padding = 'same')(conv3)
conv3 = BatchNormalization()(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2), data_format='channels_last')(conv3)

conv4 = Conv2D(n_filters * 8, (3, 3), activation='relu', padding = 'same')(pool3)
conv4 = BatchNormalization()(conv4)
conv4 = Conv2D(n_filters * 8, (3, 3), activation='relu', padding = 'same')(conv4)
conv4 = BatchNormalization()(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2), data_format='channels_last')(conv4)

conv5 = Conv2D(n_filters * 16, (3, 3), activation='relu', padding = 'same')(pool4)
conv5 = BatchNormalization()(conv5)
conv5 = Conv2D(n_filters * 16, (3, 3), activation='relu', padding = 'same')(conv5)
conv5 = BatchNormalization()(conv5)

up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=3)

conv6 = Conv2D(n_filters * 8, (3, 3), activation='relu', padding = 'same')(up6)
conv6 = BatchNormalization()(conv6)
conv6 = Conv2D(n_filters * 8, (3, 3), activation='relu', padding = 'same')(conv6)
conv6 = BatchNormalization()(conv6)

up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=3)

conv7 = Conv2D(n_filters * 4, (3, 3), activation='relu', padding = 'same')(up7)
conv7 = BatchNormalization()(conv7)
conv7 = Conv2D(n_filters * 4, (3, 3), activation='relu', padding = 'same')(conv7)
conv7 = BatchNormalization()(conv7)

up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=3)

conv8 = Conv2D(n_filters * 2, (3, 3), activation='relu', padding = 'same')(up8)
conv8 = BatchNormalization()(conv8)
conv8 = Conv2D(n_filters * 2, (3, 3), activation='relu', padding = 'same')(conv8)
conv8 = BatchNormalization()(conv8)

up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=3)

conv9 = Conv2D(n_filters * 1, (3, 3), activation='relu', padding = 'same')(up9)
conv9 = BatchNormalization()(conv9)
conv9 = Conv2D(n_filters * 1, (3, 3), activation='relu', padding = 'same')(conv9)
conv9 = BatchNormalization()(conv9)

conv10 = Conv2D(21, (1, 1), activation='softmax', padding = 'same')(conv9) # 21 = num_classes+1

model = keras.models.Model(inputs=inputs, outputs=conv10)
model.summary()
```





### ENet

***

以图像分割为例，前面提到的SegNet的速度已经相当快了，但是仍然远不能达到实时分割的目的。比如道路场景分割任务，至少需要达到10fps，而SegNet的速度只能实现1fps左右。

#### 加快速度方式

1. **特征图分辨率**

   为了减小计算量、增大感受野，许多网络都采用缩小特征图分辨率的结构（比如前面提到的SegNet）。但是，过度缩小特征图分辨率则会造成严重的信息丢失，从而造成分割精度的下降。因此，要尽可能约束下采样的比率。**目前被广泛接受的下降比率不超过1/8**。那么还要继续增大感受野该怎么办呢？没错，就是用到**空洞卷积**了。

2. **提前下采样**

   直接用原始分辨率的图片作为网络输入的代价是很高的。由于视觉信息中存在大量的冗余，在输入网络之前，可以对输入做一个预处理，也就是**先用一层网络将这些信息浓缩，同时缩小空间尺寸。**实验证明，**这一步的特征图个数不用太多，16与32效果几乎相同。**

3. **解码器规模**

   前面我们接触到的编解码结构中，解码器与编码器在结构上几乎是完全对等的。这种结构看起来没有问题，但是真的合理吗？其实，**编码器的规模可以大一些，**因为要**用来提取信息；**但是**解码器本质上只是对编码器结果的细节精调，**因此**规模可以减小。**

4. **非线性操作**

   这一点相信很多人在实验中已经发现了，那就是在某些情况下，ReLU的引入并不会对结果产生有利的影响。相反，**用PReLU替代**反而会更好。

5. **分解卷积层**

   考虑到卷积层权重其实有相当大的冗余，可以用nx1和1xn的两个卷积层级联（对称卷积）来替代一个nxn的卷积层来缩小计算量。具体地，用n=5的对称卷积的计算量近似于一个3x3的普通卷积，但是由于引入了非线性，这样的操作还能够增加函数的多样性。

6.  **空洞卷积**

   引入空洞卷积可以减小计算量、增大感受野，同时维护了特征图的分辨率。为了使空洞卷积发挥最大的作用，ENet中穿插地使用了**普通卷积、非对称卷积（Asymmetric Convolutions即1xn、nx1替换nxn）和空洞卷积（膨胀卷积）**。



**网络结构：**

![image-20200111065527324](images\图像分割.assets\image-20200111065527324.png)

图(a)对应的是ENet的初始模块，也就是前文提到的缩小输入图像分辨率，从而去除视觉冗余、减小计算量的部分；

图(b)对应的则是重复使用，从而构建网络主体的bottleneck模块。

具体结构如下表所示（输入尺寸512x512）：

![image-20200111065617581](images\图像分割.assets\image-20200111065617581.png)

其中Type一列说明的是该bottleneck对应的卷积类型，即图(b)中的conv层。





### PSPNet

***

通过优化金字塔场景解析网络，可以更好地学习场景的全局上下文表示。首先，将图像传递到基本网络以获得特征图。特征图被向下采样到不同的尺度。卷积应用于合并的特征映射。然后，所有的特征映射都被上采样到一个公共的比例，并连接在一起。最后，使用另一个卷积层生成最终的分割输出。在这里，较小的对象可以通过汇聚到高分辨率的特性很好地捕获，而大型对象则可以通过

![image-20200110034914324](images\图像分割.assets\image-20200110034914324.png)





### DeepLab系列









## 实例分割

DeepMask

Mask R-CNN

SOLO













