# 循环神经网络

**卷积神经网络**利用数据的**局部相关性**和**权值共享**的思想大大减少了网络的参数量， 非常适合于图片这种具有空间(Spatial)局部相关性的数据， 已经被成功地应用到计算机视觉领域的一系列任务上。

自然界的信号除了具有空间维度之外，还有一个**时间(Temporal)维度**。具有时间维度的信号非常常见，比如我们正在阅读的文本、 说话时发出的语音信号、 随着时间变化的股市参数等。 这类数据并不一定具有局部相关性，同时数据在时间维度上的长度也是可变的，卷积神经网络并不擅长处理此类数据。  

具有先后顺序的数据一般叫做**序列(Sequence)**



## RNN

![image-20200113031548965](./images\循环神经网络.assets\image-20200113031548965.png)

在每个时间戳 t ，网络层接受当前时间戳的输入 $x_t$  和上一个时间戳的网络状态向量  $h_{t-1}$ 经过$h_t = f_\theta(h_{t-1},x_t)$ 变换后得到当前时间戳的新状态向量$h_t$ 。

如果使用张量$W_{xh}$ 、$W_{hh}$ 、b来参数化$f_\theta$ 网络，并按照
$$
h_t = \sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)
$$
方式计算即指一般**循环神经网络**。

在循环神经网络中，激活函数更多地采用 **tanh 函数**，并且**可以选择不使用偏置b来进一步减小参数量**。  



### SimpleRNNCell

```python
# 初始化状态向量，用列表包裹，统一格式
h0 = [tf.zeros([4, 64])]
x = tf.random.normal([4, 80, 100]) # 生成输入张量， 4 个 80 单词的句子
xt = x[:,0,:] # 所有句子的第 1 个单词
# 构建输入特征 n=100,序列长度 s=80,状态长度=64 的 Cell
cell = layers.SimpleRNNCell(64)
out, h1 = cell(xt, h0) # 前向计算
print(out.shape, h1[0].shape)

Out[4]: (4, 64) (4, 64)
    
h = h0 # h 保存每个时间戳上的状态向量列表
# 在序列长度的维度解开输入(时间步)，得到 xt:[batch_size,n_dim]
for xt in tf.unstack(x, axis=1):
	out, h = cell(xt, h) # 前向计算,out 和 h 均被覆盖
# 最终输出可以聚合每个时间戳上的输出，也可以只取最后时间戳的输出
out = out

```



### SimpleRNN

```python
# 创建 RNN 层时，设置返回所有时间戳上的输出
layer = layers.SimpleRNN(64,return_sequences=True)
out = layer(x) # 前向计算
out # 输出，自动进行了 concat 操作(4,80,64)

#多层
net = keras.Sequential([ # 构建 2 层 RNN 网络,除最末层外，都需要返回所有时间戳的输出，用作下一层的输入
	layers.SimpleRNN(64, return_sequences=True),
	layers.SimpleRNN(64),
])
```



## LSTM

相对于基础的RNN网络只有一个状态向量$h_t$ ，LSTM新增了一个状态向量$C_t$ ，并引入了**门控机制**来控制信息的遗忘和刷新。

两个状态向量c和h，其中c作为LSTM的内部状态向量，h表示LSTM的输出向量。

三个门控：**输入门（Input Gate）、遗忘门（Forget Gate）、输出门（Output Gate）**。


![image-20200113034335176](images\循环神经网络.assets\image-20200113034335176.png)





### 遗忘门

遗忘门作用于LSTM状态向量$c$ 上面，**用于控制上一个时间戳的记忆$c_{t-1}$ 对当前时间戳状态$c_t$的影响。**
$$
g_f = \sigma(W_f[h_{t-1},x_t]+b_f)
$$
**一般使用Sigmoid激活函数**

当$g_f = 1$ 时，遗忘门全部打开，LSTM接受上一个状态$c_{t-1}$ 的全部信息。

当$g_f = 0$ 时，遗忘门关闭，LSTM直接忽略上一个状态$c_{t-1}$ 输出为0的向量。

经过遗忘门后，LSTM的状态向量$c_t = g_fc_{t-1}$  


![image-20200113035147209](images\循环神经网络.assets\image-20200113035147209.png)



### 输入门

**输入门用于控制 LSTM 对输入的接收程度**。 

首先通过对当前时间戳的输入$x_t$ 和上一个时间戳的输出状态$h_{t-1}$ 做非线性变换得到新的输入变量$\bar{c_t}$ 
$$
\bar{c_t}=tanh(W_c[h_{t-1},x_t]+b)
$$
**输入门的控制变量$g_i$ 决定了LSTM对当前时间戳的新输入$\bar{c_t}$ 的接受程度**
$$
gi=\sigma(W_i[h_{t-1},x_t]+b_i)
$$
**一般使用sigmoid激活函数**

当$g_i=1$时，LSTM全部接受新输入。

当$g_i=0$时，LSTM不接受任何新输入。

**经过输入门后，$c_t=g_i\bar{c_t}+g_fc_{t-1}$**  


![image-20200113040213525](images\循环神经网络.assets\image-20200113040213525.png)


  

### 输出门

和RNN不同，LSTM的内部状态向量c并不会直接用于输出，RNN状态向量h既用于记忆，又用于输出。

LSTM内部，状态向量并不会全部输出，而是在输出门作用下有选择的输出，输出变量$g_o$ 
$$
g_o = \sigma(W_o[h_{t-1},x_t]+b_o)
$$
**一般使用sigmoid激活函数**

当$g_o = 1$时，输出完全打开，LSTM状态向量$c_t$全部用于输出。

当$g_o=0$时，输出关闭，LSTM内部记忆被完全隔断，输出为0向量。 

LSTM输出$h_t$ 为：
$$
h_t = g_o*tanh(c_t)
$$



![image-20200113042914671](images\循环神经网络.assets\image-20200113042914671.png)


### LSTMCell

```python
x = tf.random.normal([2,80,100])
xt = x[:,0,:] # 得到一个时间戳的输入
cell = layers.LSTMCell(64) # 创建 LSTM Cell
# 初始化状态和输出 List,[h,c]
state = [tf.zeros([2,64]),tf.zeros([2,64])]
out, state = cell(xt, state) # 前向计算
# 查看返回元素的 id
id(out),id(state[0]),id(state[1])
Out[18]: (1537587122408, 1537587122408, 1537587122728)
    
# 在序列长度维度上解开，循环送入 LSTM Cell 单元
for xt in tf.unstack(x, axis=1):
# 前向计算
out, state = cell(xt, state)
```



### LSTM

```python
# 创建一层 LSTM 层，内存向量长度为 64
layer = layers.LSTM(64)
# 序列通过 LSTM 层，默认返回最后一个时间戳的输出 h
out = layer(x)

# 创建 LSTM 层时，设置返回每个时间戳上的输出
layer = layers.LSTM(64, return_sequences=True)
# 前向计算，每个时间戳上的输出自动进行了 concat，拼成一个张量
out = layer(x)

net = keras.Sequential([
	layers.LSTM(64, return_sequences=True), # 非末层需要返回所有时间戳输出
	layers.LSTM(64)
])
```







## GRU

LSTM 具有更长的记忆能力， 在大部分序列任务上面都取得了比基础的 RNN 模型更好的性能表现，更重要的是， LSTM 不容易出现梯度弥散现象。 

但是 LSTM 结构相对较复杂， 计算代价较高，模型参数量较大。 因此， 科学家们尝试简化 LSTM 内部的计算流程，特别是**减少门控数量**。 研究发现， **遗忘门是 LSTM 中最重要的门控** ， 甚至发现只有遗忘门的简化版网络在多个基准数据集上面优于标准 LSTM 网络。 在众多的简化版 LSTM中，**门控循环网络(Gated Recurrent Unit，简称 GRU)**是应用最广泛的 RNN 变种之一。 GRU把内部状态向量和输出向量合并，统一为状态向量 ， 门控数量也减少到 2 个：**复位门(Reset Gate)**和**更新门(Update Gate)**


![image-20200113044509364](images\循环神经网络.assets\image-20200113044509364.png)




### 复位门

**复位门用于控制上一个时间戳的状态$h_{t-1}$ 进入GRU的量。**     
$$
g_r = \sigma(W_r[h_{t-1},x_t]+b_r)
$$
**一般使用sigmoid激活函数**

新输入$\bar{h_t}$ :
$$
\bar{h_t}=tanh(W_h[g_rh_{t-1},x_t]+b_h)
$$
当$g_r=0$时，新输入全部来自与 $x_t$ ，不接受$h_{t-1}$ 相当于复位$h_{t-1}$ 。

当$g_r=1$时，$h_{t-1}$和$x_t$ 共同产生新输入。


![image-20200113044929476](images\循环神经网络.assets\image-20200113044929476.png)




### 更新门

**更新门用控制上一时间戳状态 $h_{t-1}$ 和新输入$\bar{h_t}$ 对新状态向量$h_t$ 的影响程度。**
$$
g_z = \sigma(W_z[h_{t-1},x_t]+b_z)
$$
**一般使用sigmoid激活函数**

**$g_z$用于控制新输入$\bar{h_t}$，$1-g_z$用于 控制状态$h_{t-1}$ ,**新输出$h_t$ :
$$
h_t = (1-g_z)h_{t-1} + g_z\bar{h_t}
$$
当$g_z=0$时，$h_t$全部来自上一个时间戳状态。

当$g_z=1$时，$h_t$全部来自新输入。


![image-20200113045852665](images\循环神经网络.assets\image-20200113045852665.png)




### GRUCell

用法同LSTMCell

### GRU

用法同LSTM





## BRNN（双向）

有些时候预测可能需要由前面若干输入和后面若干输入共同决定，这样会更加准确。因此提出了**双向循环神经网络**.

BRNN有两个RNN上下叠加在一起组成的，输出由这两个RNN的状态共同决定。


![image-20200113051643468](images\循环神经网络.assets\image-20200113051643468.png)




```python
# Bidirectional 参数 layer,merge_mode,backward_layer
# merge_mode: Mode by which outputs of the forward and backward RNNs will be combined. One of 
# {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be 
# returned as a list.
model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

 # With custom backward layer
 model = Sequential()
 forward_layer = LSTM(10, return_sequences=True)
 backward_layer = LSTM(10, activation='relu', return_sequences=True,go_backwards=True)
 model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                         input_shape=(5, 10)))
 model.add(Dense(5))
 model.add(Activation('softmax'))
 model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
```





  





















