# 目标检测

## 算法分类

**目标检测（Object Detection）**的任务是找出图像中所有感兴趣的目标，确定它们的**类别**和**位置**，是计算机视觉领域的核心问题之一。

目前学术和工业界出现的目标检测算法分成3类：

1. **传统的目标检测算法**：**Cascade + HOG/DPM + Haar/SVM**以及上述方法的改进、优化；
2. **候选区域 + 深度学习分类（Two stage）**：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案：**R-CNN系列**等
3. **基于深度学习的回归方法（One stage）**：**YOLO/SSD/DenseBox** 等

https://github.com/amusi/awesome-object-detection



## 基本组件

**边界框bounding box**





## R-CNN

预先找出图中目标可能出现的位置，即**候选区域（Region Proposal）**，利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。选定候选框Region Proposal的方法，**选择性搜索（Selective Search）**

**具体步骤则如下**

一、训练一个分类模型（比如AlexNet）

二、对该模型做fine-tuning

​	1、将分类数从1000改为21，比如20个物体类别 + 1个背景

​	2、去掉最后一个全连接层

三、特征提取

​	1、提取图像的所有候选框（选择性搜索Selective Search）

​	2、对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘

四：训练一个SVM分类器（二分类）来判断这个候选框里物体的类别。

​	  每个类别对应一个SVM，判断是不是属于这个类别，是就是positive，反之nagative。

五：使用回归器精细修正候选框位置：对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。



**存在问题**

R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。

有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。

但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。



### SPP-Net

**SPP：Spatial Pyramid Pooling（空间金字塔池化）**

当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行**crop**等一系列操作以统一图片的尺寸大小，比如224x224（ImageNet）、32x32(LeNet)、96x96等（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227

但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。

那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？

这个结构就是**spatial pyramid pooling layer**



**SPP的特点有两个:**

**1、**在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。

SPP Net在普通的CNN结构中加入了**ROI池化层（ROI Pooling）**，使得网络的输入图像可以是任意尺寸的，输出则是一个固定维数的向量。

ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。

**2、**只对原图提取一次卷积特征

在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。

而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的**卷积特征feature map**，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。



**ROI pooling**

（1）根据输入image，将ROI映射到feature map对应位置；

（2）将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）；

（3）对每个sections进行max pooling操作；

例如：一个8x8大小的feature map，一个ROI，输出大小为2x2.

![image-20200109044751910](images\目标检测.assets\image-20200109044751910.png)



### Fast RCNN

Fast R-CNN在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。

与R-CNN框架图对比，主要有两处不同：

一、最后一个卷积层后加了一个**ROI pooling layer**，

二、损失函数使用了**分类加回归多任务损失函数(multi-task loss)**，将边框回归Bounding Box Regression直接加入到CNN网络中训练。



### Faster RCNN

Fast RCNN存在的问题：选择性搜索，找出所有的候选框，这个也非常耗时。

所以，在Faster RCNN中引入**Region Proposal Network(RPN)**替代Selective Search，同时引入**anchor box**应对目标形状的变化。

![image-20200109045125063](images\目标检测.assets\image-20200109045125063.png)

**RPN(Region Proposal Network)**

考虑9个可能的候选窗口：三种面积{128,256,512}×三种比例{1:1,1:2,2:1}。这些候选窗口称为**anchors**。

![image-20200109045423205](images\目标检测.assets\image-20200109045423205.png)

**RPN步骤：**

得到9个anchor，我们给每个anchor分配一个二进制的标签（前景:1,背景:0）。

我们分配正标签给两类anchor：

**1）**与某个**ground truth（GT）**包围盒有最高的IoU重叠的anchor（也许不到0.7），

**2）**与任意GT有大于0.7的IoU交叠的anchor。

注意到一个GT包围盒可能分配正标签给多个anchor。我们分配负标签给与所有GT包围盒的IoU比率都低于0.3的anchor。非正非负的anchor对训练目标没有任何作用，由此输出维度为2x9=18，一共18维。

假设在feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分前景和背景，所以每个点转化为cls=2k 分类scores；而每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k 回归坐标(coordinates)

**3）**边框回归：已经计算出前景anchors，使用bounding box regression得到预设anchor-box到ground-truth-box之间的变换参数，即平移（dx和dy）和伸缩参数（dw和dh），由此得到初步确定proposal。

对于窗口一般使用四维向量(x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的预测窗口G'，

**即：给定anchor A=(Ax, Ay, Aw, Ah)，寻找映射f，使得f(Ax, Ay, Aw, Ah)=(G'x, G'y, G'w, G'h)，使(G'x, G'y, G'w, G'h)≈(Gx, Gy, Gw, Gh)。**

**4）**将预测的proposal将anchors映射回原图，判断预测的proposal是否大范围超过边界，剔除严重超出边界的。

**5）**按照分类score进行从大到小排序，提取前2000个预proposal，对这个2000个进行**NMS(非极大值抑制)**，将得到的再次进行排序，输出300个proposal。



## SSD
***

![image-20200109222733184](images\目标检测.assets\image-20200109222733184.png)

### DSSD
***

![image-20200110005948673](images\目标检测.assets\image-20200110005948673.png)

### DSOD
***

![image-20200110012304750](images\目标检测.assets\image-20200110012304750.png)



### FSSD
***

借鉴FPN的思想，重构一组特征图金字塔，使算法精度明显提升，速度没有下降太多。

![image-20200110013656284](images\目标检测.assets\image-20200110013656284.png)



### RSSD

***



![image-20200110015218210](images\目标检测.assets\image-20200110015218210.png)



## RetinaNet

***





## Yolo系列

***





# 文本检测和识别

一般来说，从场景图片中进行文字识别，需要包括2个步骤：

- **文字检测**：解决的问题是哪里有文字，文字的范围有多少
- **文字识别**：对定位好的文字区域进行识别，主要解决的问题是每个文字是什么，将图像中的文字区域进转化为字符信息。



基于RNN**文字识别**算法主要有两个框架：

1. **CNN+RNN+CTC(CRNN+CTC)**
2. **CNN+Seq2Seq+Attention**



## 数据集

https://www.cnblogs.com/lillylin/p/6893500.html

![image-20200113055522770](images\目标检测.assets\image-20200113055522770.png)



## CRNN

***

https://arxiv.org/pdf/1507.05717.pdf

全称**Convolutional Recurrent Neural Network，**主要用于端到端地对**不定长**的文本序列进行识别，不用先对单个文字进行切割，而是将文本识别转化为时序依赖的序列学习问题，就是基于图像的序列识别。

整个CRNN网络结构包含三部分，从下到上依次为：

1. CNN（卷积层），使用深度CNN，对输入图像提取特征，得到**特征图**；
2. RNN（循环层），使用双向RNN（BLSTM）对**特征序列**进行预测，对序列中的每个特征向量进行学习，并输出预测标签（真实值）分布；
3. CTC loss（转录层），使用 CTC 损失，把从循环层获取的一系列标签分布转换成最终的标签序列。

![image-20200113015353844](images\目标检测.assets\image-20200113015353844.png)



**1、CNN**

![image-20200113015938112](images\目标检测.assets\image-20200113015938112.png)

这里有一个很精彩的改动，一共有四个最大池化层，但是最后两个池化层的窗口尺寸由 2x2 改为 1x2，也就是图片的高度减半了四次（除以 ![[公式]](https://www.zhihu.com/equation?tex=2%5E4) ），而宽度则只减半了两次（除以 ![[公式]](https://www.zhihu.com/equation?tex=2%5E2) ），这是因为文本图像多数都是高较小而宽较长，所以其feature map也是这种高小宽长的矩形形状，如果使用1×2的池化窗口可以尽量保证不丢失在宽度方向的信息，更适合英文字母识别（比如区分i和l）。

**输入图像**高度为32，这是固定的，图片通过 CNN 后，高度就变为1，这点很重要；假设输入图像大小为 ![[公式]](https://www.zhihu.com/equation?tex=%2832%2C+100%2C3%29)，CNN将大小为 ![[公式]](https://www.zhihu.com/equation?tex=%2832%2C+100%2C3%29) 的图像转换为 ![[公式]](https://www.zhihu.com/equation?tex=%281%2C25%2C512%29) 大小的卷积特征图。

可以做如下处理：

- 首先会将图像在固定长宽比的情况下缩放到 ![[公式]](https://www.zhihu.com/equation?tex=32%5Ctimes+W%5Ctimes3) 大小（ ![[公式]](https://www.zhihu.com/equation?tex=W) 代表任意宽度）
- 然后经过CNN后变为 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes+%28W%2F4%29%5Ctimes512)
- 针对LSTM设置 ![[公式]](https://www.zhihu.com/equation?tex=T%3D%28W%2F4%29) ，即可将特征输入LSTM。

所以在处理输入图像的时候，建议在保持长宽比的情况下将高缩放到 ![[公式]](https://www.zhihu.com/equation?tex=32)，这样能够尽量不破坏图像中的文本细节（当然也可以将输入图像缩放到固定宽度，但是这样由于破坏文本的形状，肯定会造成性能下降）。



**2、RNN**

由于卷积层，最大池化层和激活函数在局部区域上执行，因此它们是平移不变的。因此，特征图的每列（即一个特征向量）对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。特征序列中的每个向量关联一个感受野。

这些特征向量序列就作为循环层的输入，每个特征向量作为 RNN 在一个时间步（time step）的输入。

![image-20200113021332834](images\目标检测.assets\image-20200113021332834.png)

由于CNN输出的Feature map是![[公式]](https://www.zhihu.com/equation?tex=%281%2C25%2C512%29)大小，所以对于RNN最大时间长度 ![[公式]](https://www.zhihu.com/equation?tex=T%3D25) （即有25个时间输入，每个输入 ![[公式]](https://www.zhihu.com/equation?tex=x_t) 列向量有 ![[公式]](https://www.zhihu.com/equation?tex=D%3D512) ）。



因为 RNN 有梯度消失的问题，不能获取更多上下文信息，所以 CRNN 中使用的是 **LSTM**。

LSTM 是单向的，它只使用过去的信息。然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。

**将两个LSTM，一个向前和一个向后组合到一个双向LSTM中**。此外，可以堆叠多层双向LSTM，深层结构允许比浅层抽象更高层次的抽象。



**3、CTC**

在语音识别中，我们的数据集是音频文件和其对应的文本，不幸的是，音频文件和文本很难再单词的单位上对齐。除了语言识别，在OCR，机器翻译中，都存在类似的Sequence to Sequence结构，同样也需要在预处理操作时进行对齐，但是这种对齐有时候是非常困难的。如果不使用对齐而直接训练模型时，由于人的语速的不同，或者字符间距离的不同，导致模型很难收敛。

CTC是一种Loss计算方法，训练样本无需对齐。CTC特点：

- 引入blank字符，解决有些位置没有字符的问题
- 通过递推，快速计算梯度

对于Recurrent Layers，如果使用常见的Softmax cross-entropy loss，则每一列输出都需要对应一个字符元素。那么训练时候每张样本图片都需要标记出每个字符在图片中的位置，再通过CNN感受野对齐到Feature map的每一列获取该列输出对应的Label才能进行训练。

在实际情况中，标记这种对齐样本非常困难（除了标记字符，还要标记每个字符的位置），工作量非常大。另外，由于每张样本的字符数量不同，字体样式不同，字体大小不同，导致每列输出并不一定能与每个字符一一对应。

当然这种问题同样存在于语音识别领域。例如有人说话快，有人说话慢，那么如何进行语音帧对齐，是一直以来困扰语音识别的巨大难题。

所以CTC提出一种对不需要对齐的Loss计算方法，用于训练网络，被广泛应用于文本行识别和语音识别中。



**序列合并机制**

我们现在要将 RNN 输出的序列翻译成最终的识别结果，RNN进行时序分类时，不可避免地会出现很多冗余信息，比如一个字母被连续识别两次，这就需要一套去冗余机制。

![image-20200114162709658](images\目标检测.assets\image-20200114162709658.png)

比如我们要识别上面这个文本，其中 RNN 中有 5 个时间步，理想情况下 t0, t1, t2 时刻都应映射为“a”，t3, t4 时刻都应映射为“b”，然后将这些字符序列连接起来得到“aaabb”，我们再将连续重复的字符合并成一个，那么最终结果为“ab”。

这似乎是个比较好的方法，但是存在一个问题，如果是book，hello之类的词，合并连续字符后就会得到 bok 和 helo，这显然不行，所以 CTC 有一个blank机制来解决这个问题。

我们以“-”符号代表blank，**RNN 输出序列时，在文本标签中的重复的字符之间插入一个“-”**，比如输出序列为“bbooo-ookk”，则最后将被映射为“book”，即有blank字符隔开的话，连续相同字符就不进行合并。

即对字符序列先删除连续重复字符，然后从路径中删除所有“-”字符，这个称为解码过程，而编码则是由神经网络来实现。引入blank机制，我们就可以很好地解决重复字符的问题。

相同的文本标签可以有多个不同的字符对齐组合，例如，“aa-b”和“aabb”以及“-abb”都代表相同的文本(“ab”)，但是与图像的对齐方式不同。更总结地说，一个文本标签存在一条或多条的路径。



####　tf.nn.ctc_loss

```python
tf.nn.ctc_loss(
    labels,
    logits,
    label_length,
    logit_length,
    logits_time_major=True,
    unique=None,
    blank_index=None,
    name=None
)
```

**Notes:**

1、标签可以是密集的、零填充的张量，带有标签序列长度的矢量，也可以作为SparseTensor。

2、在TPU和GPU上：只支持密集的填充标签。

3、在CPU上：调用者可以使用SparseTensor或稠密的填充标签，但是使用SparseTenser调用将大大加快速度。



**Args:**

- **`labels`**: tensor of shape **[batch_size, max_label_seq_length]** or SparseTensor

- **`logits`**: tensor of shape **[frames, batch_size, num_labels]**, if logits_time_major == False, shape is [batch_size, frames, num_labels].

- **`label_length`**: tensor of shape **[batch_size]** `None if labels is SparseTensor` Length of reference label sequence in labels.
- **`logit_length`**: tensor of shape **[batch_size]** Length of input sequence in logits.
- **`logits_time_major`**: (optional) If True (default), logits is shaped [time, batch, logits]. If False, shape is [batch, time, logits]
- **`unique`**: (optional) Unique label indices as computed by ctc_unique_labels(labels). If supplied, enable a faster, memory efficient implementation on TPU.
- **`blank_index`**: (optional) Set the class index to use for the blank label. Negative values will start from num_classes, ie, -1 will reproduce the ctc_loss behavior of using **num_classes - 1** for the blank symbol. There is some memory/performance overhead to switching from the default of 0 as an additional shifted copy of the logits may be created.
- **`name`**: A name for this `Op`. Defaults to "ctc_loss_dense".



**Returns:**

- **`loss`**: tensor of shape **[batch_size]**, negative log probabilities.















